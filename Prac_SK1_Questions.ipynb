{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Practice Exercise: Scikit-Learn 1\n",
    "## Basic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In line with the [SK1 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-1-basic-modeling/), the objective of this practice notebook is to familiarize you with working with a regression problem using a `holdout` approach. The dataset under consideration is the `diamonds` dataset that comes with the `ggplot2` library in R.\n",
    "\n",
    "The `diamonds` dataset contains information on diamonds including carat (numeric), clarity (categorical), cut (categorical), and color (categorical). The dataset has 10 features and 53940 instances. The objective is to predict the price of a diamond in USD given its attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 0: Data Preparation\n",
    "\n",
    "Prepare the dataset for predictive modeling as follows:\n",
    "\n",
    "0. Refer to our data prep practice solutions on Canvas as well as our data prep script on GitHub [here](https://github.com/vaksakalli/datasets/blob/master/prepare_dataset_for_modeling_github.py) for some inspiration on preparing this data for predictive modeling.\n",
    "1. Set `pd.set_option('display.max_columns', None)`. Read in the raw data, which is named as `diamonds.csv` on GitHub. Have a look at the shape and data types of the features. Also have a look at the top 5 rows.\n",
    "2. Generate descriptive statistics for categorical and numerical features separately.\n",
    "3. Have a look at the unique values for each categorical feature and check whether everything is OK in the sense that there are no unusual values.\n",
    "4. Make sure there are no missing values anywhere.\n",
    "5. Separate the last column from the dataset and set it to \"target\". Make sure \"target\" is a `Pandas` series at this point, and not a `NumPy` array (which will be necessary for the sampling below). Set all the other columns to be the \"Data\" data frame, which will be the set of descriptive features.\n",
    "6. Make sure all categorical descriptive features are encoded via one-hot-encoding. In this particular dataset, some categorical descriptive features are actually ordinal, but we will go ahead and encode them via one-hot-encoding for simplicity.\n",
    "7. Make sure all descriptive features are scaled via min-max scaling and the output is a `Pandas` data frame with correct column names. Do **NOT** scale the target feature!\n",
    "8. Finally have a look at the top 5 rows of \"target\" and \"Data\" respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1: Modeling Preparation\n",
    "\n",
    "- Randomly sample 5000 rows as it's too big for a short demo (using a random seed of 999). Make sure to run `reset_index(drop=True)` on the sampled data to reset the indices.\n",
    "> - **NOTE:** It's **extremely** important to use the same seed for both Data and target while sampling, otherwise you will happily mix and match different rows without getting any execution errors and all you results will be garbage.\n",
    "- Split the sampled data as 70% training set and the remaining 30% test set using a random seed of 999. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "- Fit a nearest neighbor (NN) regressor with $k=3$ neighbors using the Euclidean distance. \n",
    "- Fit the model on the train data and evaluate its $R^2$ (the default \"score()\" for regressors) performance on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- Extend Question 2 by fitting $k=1,\\ldots,10$ neighbors using the Manhattan and Euclidean distances respectively.\n",
    "- What is the optimal $k$ value for each distance metric? That is, at which $k$, the NN regressor returns the highest $R^2$ score?\n",
    "- Which distance metric seems to be better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "- Fit a decision tree regressor with default values on the train data, and then evaluate its performance on the test data. \n",
    "- Does it perform better than the best KNN model from the previous question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "- Fit a simple linear regression model on train data, and then evaluate its performance on the test data. **Hint:** Use `LinearRegression()` in `sklearn.linear_model`. \n",
    "- How does it compare to the previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "- Fit a random forest regressor with `n_estimators=100` on train data, and then evaluate its performance on the test data. \n",
    "- How does it compare to the previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "- Predict the first 5 observations of the **test** data using the linear regression model you built earlier. \n",
    "- Display your results as a data frame with three columns: 'target', 'prediction', 'absolute_diff'.\n",
    "- How do the predictions look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Exercise 8\n",
    "\n",
    "This question requires basic knowledge of linear regression:\n",
    "- Compute the previous predictions directly *without* using the `fit()` function. \n",
    "\n",
    "**Background:** Using the textbook notation, a linear regression model is defined as:\n",
    "\n",
    "$$\\mathbb{M}_w(d) = w[0] + \\sum_{j=1}^{p} w[j] \\times d[j]$$\n",
    "\n",
    "where \n",
    "- $d$ is a vector of descriptive features, \n",
    "- $w[0]$ is the intercept, and\n",
    "- $w[1], w[2],\\ldots,w[p]$ are the regression weights corresponding to these features. \n",
    "\n",
    "Here, the target feature (the price of a diamond) can be modeled as a weighted linear combination of descriptive features. We shall not cover linear regression in depth. If you are interested, please refer to Sections 7.2 and 7.3 in the textbook for more details.\n",
    "\n",
    "**Hint:** \n",
    "1. Use the `coef_` attribute of the `linear_regressor` object to get the regression coefficients. \n",
    "2. Multiply `new_obs` with the regression coefficients, perform sum along `axis=1`.\n",
    "3. Use the `intercept_` attribute to get the constant term in the linear regression and add it to the above sum. \n",
    "4. Confirm that your results are the same as in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
